# GLM-4.7 - Instala√ß√£o e Configura√ß√£o para PC Fraco

Este reposit√≥rio cont√©m scripts automatizados para baixar, instalar e testar modelos locais em m√°quinas com recursos limitados.

Atualmente, os scripts est√£o prontos para uso com **GLM-4.7** (GGUF/llama.cpp) e modelos **Ollama** (Qwen, GLM leve, DeepSeek, Codestral), com foco em validar se o PC aguenta cada perfil de modelo.

## üìã Requisitos M√≠nimos

### Hardware M√≠nimo (CPU Only)

- **RAM**: 32GB+ (recomendado 64GB+)
- **Disco**: 200GB+ de espa√ßo livre (SSD recomendado)
- **CPU**: Processador multi-core moderno

### Hardware Recomendado (com GPU)

- **GPU**: 8GB+ VRAM (recomendado 16GB+)
- **RAM**: 64GB+ (recomendado 128GB+)
- **Disco**: 300GB+ de espa√ßo livre (NVMe SSD recomendado)
- **CUDA**: Compat√≠vel com CUDA 11.8+ ou 12.1+

## üéØ Vers√µes de Modelo Dispon√≠veis

O GLM-4.7 est√° dispon√≠vel em v√°rias quantiza√ß√µes para diferentes capacidades de hardware:

| Vers√£o | Tamanho | RAM M√≠nima | VRAM M√≠nima | Uso Recomendado |
|--------|---------|------------|-------------|-----------------|
| **UD-Q2_K_XL** (2-bit) | ~135GB | 128GB | 24GB | M√°quinas potentes com GPU |
| **Q4_K_M** (4-bit) | ~200GB | 64GB | 16GB | M√°quinas moderadas |
| **Q4_K_S** (4-bit) | ~180GB | 48GB | 12GB | M√°quinas modestas |
| **Q5_K_M** (5-bit) | ~240GB | 80GB | 20GB | Melhor qualidade |

## üß† Comparativo Pr√°tico para Codifica√ß√£o

Crit√©rio de escolha entre fam√≠lias de modelos para desenvolvimento local:

| Crit√©rio | Qwen3-Coder (32B/80B MoE) | GLM-4.7 (Reasoning) | MiniMax M2.1 |
|----------|----------------------------|---------------------|--------------|
| Ponto forte | L√≥gica pura e algoritmos | Arquitetura e documenta√ß√£o | Velocidade e itera√ß√£o de agentes |
| Python | SOTA (state-of-the-art) | Excelente | Muito bom |
| React/Angular | C√≥digo funcional, mas seco | Superior (UI/UX e componentiza√ß√£o) | R√°pido, mas √†s vezes incompleto |
| Java / .NET | √ìtimo em m√©todos isolados | L√≠der em padr√µes corporativos | Bom para refatora√ß√£o r√°pida |
| ‚ÄúVibe‚Äù Opus | ‚ÄúG√™nio matem√°tico‚Äù | ‚ÄúArquiteto s√™nior‚Äù | ‚ÄúDev j√∫nior muito r√°pido‚Äù |

> Observa√ß√£o: o quadro acima √© um guia pr√°tico de escolha por perfil de tarefa. Neste reposit√≥rio, voc√™ pode usar tanto modelos GGUF (Hugging Face + llama.cpp) quanto modelos Ollama (Qwen/GLM/DeepSeek/Codestral).

## üöÄ In√≠cio R√°pido

### Windows (PowerShell)

```powershell
# 1. Instalar depend√™ncias
.\scripts\install.ps1

# 2. Ver modelos dispon√≠veis
.\scripts\download-model.ps1 -List

# 3A. Baixar modelo Ollama (ex.: Qwen3)
.\scripts\download-model.ps1 -Version QWEN3_CODER_OLLAMA

# 3B. Baixar modelo Ollama 14B leve (Qwen 2.5 Coder 14B)
.\scripts\download-model.ps1 -Version QWEN_CODER_14B_OLLAMA

# 3C. Baixar modelo GGUF GLM-4.7
.\scripts\download-model.ps1 -Version Q4_K_S -OutputDir .\models

# 4A. Rodar modelo Ollama pelo script
.\scripts\run-ollama.ps1 -ModelVersion QWEN3_CODER_OLLAMA

# 4B. Rodar modelo GGUF no llama.cpp
.\scripts\run-llamacpp.ps1
```

### Linux/Mac (Bash)

```bash
# 1. Instalar depend√™ncias
chmod +x scripts/install.sh
./scripts/install.sh

# 2. Baixar modelo GGUF GLM-4.7
chmod +x scripts/download-model.sh
./scripts/download-model.sh Q4_K_S

# 3. Executar GGUF com llama.cpp
chmod +x scripts/run-llamacpp.sh
./scripts/run-llamacpp.sh

# 4. Ver modelos Ollama e rodar um modelo
chmod +x scripts/run-ollama.sh
./scripts/run-ollama.sh --list
./scripts/run-ollama.sh QWEN3_CODER_OLLAMA
```

## üß™ Comandos Prontos (Copy/Paste)

### 1) Lista de chaves de modelo

Na raiz do projeto:

```powershell
.\scripts\download-model.ps1 -List
.\scripts\run-ollama.ps1 -List
```

Se estiver dentro de `scripts`:

```powershell
.\download-model.ps1 -List
.\run-ollama.ps1 -List
```

### 2) Download de modelos Ollama (Windows)

```powershell
# Qwen3-Coder (latest)
.\scripts\download-model.ps1 -Version QWEN3_CODER_OLLAMA

# Qwen 2.5 Coder 14B (leve/intermedi√°rio)
.\scripts\download-model.ps1 -Version QWEN_CODER_14B_OLLAMA

# Qwen 2.5 Coder 7B
.\scripts\download-model.ps1 -Version QWEN_CODER_7B_OLLAMA

# GLM-4 9B (leve)
.\scripts\download-model.ps1 -Version GLM_4_9B_OLLAMA

# DeepSeek Coder V2 16B
.\scripts\download-model.ps1 -Version DEEPSEEK_CODER_V2_16B_OLLAMA

# Codestral 22B
.\scripts\download-model.ps1 -Version CODESTRAL_22B_OLLAMA
```

### 2.1) Teste r√°pido (Qwen 7B vs GLM 4 9B)

```powershell
# Baixar os dois modelos para compara√ß√£o
.\scripts\download-model.ps1 -Version QWEN_CODER_7B_OLLAMA
.\scripts\download-model.ps1 -Version GLM_4_9B_OLLAMA

# Rodar Qwen Coder 7B
.\scripts\run-ollama.ps1 -ModelVersion QWEN_CODER_7B_OLLAMA

# Rodar GLM 4 9B
.\scripts\run-ollama.ps1 -ModelVersion GLM_4_9B_OLLAMA
```

### 3) Download de modelos GGUF (Hugging Face)

```powershell
.\scripts\download-model.ps1 -Version Q4_K_S -OutputDir .\models
.\scripts\download-model.ps1 -Version Q4_K_M -OutputDir .\models
.\scripts\download-model.ps1 -Version Q5_K_M -OutputDir .\models
.\scripts\download-model.ps1 -Version UD-Q2_K_XL -OutputDir .\models
```

### 4) Rodar os modelos com facilidade

```powershell
# Rodar via script (Ollama)
.\scripts\run-ollama.ps1 -ModelVersion QWEN3_CODER_OLLAMA

# Rodar via Ollama direto
ollama run qwen3-coder

# Rodar GGUF com llama.cpp
.\scripts\run-llamacpp.ps1
```

### 5) Se o comando `ollama` n√£o for reconhecido

```powershell
# Validar vers√£o por caminho absoluto
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" --version

# Listar modelos instalados
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" list

# Rodar modelo direto
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" run qwen3-coder
```

### 6) Modelos no Git

- Para modelos GGUF, use uma pasta local como `models/` e adicione no `.gitignore`.
- Para modelos Ollama, os arquivos ficam no storage do Ollama (fora da pasta do projeto por padr√£o).

## üìÅ Estrutura do Reposit√≥rio

```
.
‚îú‚îÄ‚îÄ README.md                 # Este arquivo
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ install.sh            # Instala√ß√£o Linux/Mac
‚îÇ   ‚îú‚îÄ‚îÄ install.ps1           # Instala√ß√£o Windows
‚îÇ   ‚îú‚îÄ‚îÄ download-model.sh     # Download modelo (Linux/Mac)
‚îÇ   ‚îú‚îÄ‚îÄ download-model.ps1    # Download modelo (Windows)
‚îÇ   ‚îú‚îÄ‚îÄ run-llamacpp.sh       # Executar com llama.cpp (Linux/Mac)
‚îÇ   ‚îú‚îÄ‚îÄ run-llamacpp.ps1      # Executar com llama.cpp (Windows)
‚îÇ   ‚îú‚îÄ‚îÄ run-ollama.sh         # Executar com Ollama (Linux/Mac)
‚îÇ   ‚îî‚îÄ‚îÄ run-ollama.ps1        # Executar com Ollama (Windows)
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ hardware-config.yaml  # Configura√ß√£o de hardware
‚îÇ   ‚îî‚îÄ‚îÄ model-config.json     # Configura√ß√µes do modelo
‚îî‚îÄ‚îÄ models/                   # Diret√≥rio para modelos baixados
```

## ‚öôÔ∏è Configura√ß√£o

### 1. Configurar Hardware

Edite `config/hardware-config.yaml` com as especifica√ß√µes da sua m√°quina:

```yaml
hardware:
  gpu:
    available: true
    vram_gb: 8
    cuda_arch: "75"  # Para RTX 2060, 2070, 2080
  ram_gb: 32
  cpu_cores: 8
  disk_space_gb: 500
```

### 2. Escolher Vers√£o do Modelo

Baseado no seu hardware, escolha a vers√£o adequada:

- **PC muito fraco (32GB RAM, sem GPU)**: Use `Q4_K_S` ou considere modelos menores
- **PC moderado (64GB RAM, GPU 8-16GB)**: Use `Q4_K_M`
- **PC potente (128GB+ RAM, GPU 24GB+)**: Use `UD-Q2_K_XL` ou `Q5_K_M`

## üîß M√©todos de Execu√ß√£o

### Op√ß√£o 1: llama.cpp (Recomendado para hardware limitado)

O `llama.cpp` oferece melhor controle sobre offloading CPU/GPU e quantiza√ß√£o.

**Vantagens:**

- Suporte a offloading inteligente
- Menor uso de mem√≥ria
- Melhor para hardware limitado

### Op√ß√£o 2: Ollama (Mais simples)

O Ollama √© mais f√°cil de usar, mas pode ser menos eficiente em hardware limitado.

**Vantagens:**

- Instala√ß√£o mais simples
- Interface mais amig√°vel
- Gerenciamento autom√°tico de modelos

## üìù Exemplos de Uso

### Executar com contexto pequeno (economiza mem√≥ria)

```bash
./scripts/run-llamacpp.sh --ctx-size 4096 --threads 4
```

### Executar apenas em CPU

```bash
./scripts/run-llamacpp.sh --cpu-only
```

### Executar com offloading parcial para CPU

```bash
./scripts/run-llamacpp.sh --gpu-layers 10
```

## üêõ Solu√ß√£o de Problemas

### Erro: "Out of memory"

- Reduza o `--ctx-size` (tamanho do contexto)
- Use uma vers√£o mais quantizada do modelo
- Reduza `--gpu-layers` para fazer mais offload para CPU

### Erro: "CUDA not found"

- Verifique se o CUDA est√° instalado: `nvidia-smi`
- Recompile o llama.cpp com suporte CUDA

### Modelo muito lento

- Aumente `--threads` (n√∫mero de threads CPU)
- Use mais camadas na GPU se tiver VRAM dispon√≠vel
- Considere usar uma vers√£o mais leve do modelo

## üìö Recursos Adicionais

- [Documenta√ß√£o oficial GLM-4.7](https://huggingface.co/zai-org/GLM-4.7)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [Ollama Documentation](https://ollama.ai/docs)
- [Modelos quantizados no Hugging Face](https://huggingface.co/bartowski/zai-org_GLM-4.7-GGUF)

## üìÑ Licen√ßa

Este reposit√≥rio cont√©m scripts de instala√ß√£o e configura√ß√£o. O modelo GLM-4.7 possui sua pr√≥pria licen√ßa - consulte o reposit√≥rio oficial.

## ü§ù Contribui√ß√µes

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para abrir issues ou pull requests.

## ‚ö†Ô∏è Avisos

- Modelos grandes podem demorar muito para baixar (100GB+)
- A primeira execu√ß√£o pode ser lenta enquanto o modelo carrega
- Certifique-se de ter espa√ßo em disco suficiente antes de baixar
- Em hardware muito limitado, considere usar modelos menores ou servi√ßos em nuvem
